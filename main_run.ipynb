{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 666\n",
    "\n",
    "import os, warnings, sys, six\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "\n",
    "from dask_ml.model_selection import RandomizedSearchCV\n",
    "\n",
    "from project.utils import (\n",
    "    custome_read_data, \n",
    "    make_metrics_for_tuning, \n",
    "    evalute_test_data,\n",
    "    save_results\n",
    ")\n",
    "\n",
    "from project.models import (\n",
    "    get_encoders,\n",
    "    get_ml_algo,\n",
    "    get_oversampler,\n",
    "    get_imputer,\n",
    ")\n",
    "\n",
    "from project.pipelines import make_main_pipeline\n",
    "\n",
    "pd.set_option(\"max_rows\", 100)\n",
    "pd.set_option(\"max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = custome_read_data()\n",
    "\n",
    "h1n1_data = data_dict[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine class label imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dont receive vaccine 78.75 % of the dataset\n",
      "\n",
      "Receive vaccine 21.25 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Dont receive vaccine\",\n",
    "    round(h1n1_data[\"label\"].compute().value_counts()[0] / len(h1n1_data) * 100, 2),\n",
    "    \"% of the dataset\\n\",\n",
    ")\n",
    "print(\n",
    "    \"Receive vaccine\",\n",
    "    round(h1n1_data[\"label\"].compute().value_counts()[1] / len(h1n1_data) * 100, 2),\n",
    "    \"% of the dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting train test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA SHAPE:  (18694, 35) \n",
      "\n",
      "TEST DATA SHAPE:  (8013, 35) \n",
      "\n",
      "Label Distributions: \n",
      "\n",
      "Train : [0.78752541 0.21247459] \n",
      "\n",
      "Test : [0.78759516 0.21240484]\n"
     ]
    }
   ],
   "source": [
    "h1n1_copy = h1n1_data.compute().copy()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    h1n1_copy.drop(\"label\", axis=1),\n",
    "    h1n1_copy[\"label\"],\n",
    "    test_size=0.30,\n",
    "    random_state=random_seed,\n",
    "    stratify=h1n1_copy[\"label\"],\n",
    "    shuffle=True,\n",
    ")\n",
    "print(\"TRAIN DATA SHAPE: \", x_train.shape, \"\\n\")\n",
    "print(\"TEST DATA SHAPE: \", x_test.shape, \"\\n\")\n",
    "\n",
    "train_unique_label, train_counts_label = np.unique(y_train, return_counts=True)\n",
    "test_unique_label, test_counts_label = np.unique(y_test, return_counts=True)\n",
    "\n",
    "x_train_copy = x_train.copy()\n",
    "y_train_copy = y_train.copy()\n",
    "\n",
    "print(\"Label Distributions: \\n\")\n",
    "print(\"Train :\", train_counts_label / len(y_train), \"\\n\")\n",
    "print(\"Test :\", test_counts_label / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(\n",
    "    scores,\n",
    "    ml_models_dict,\n",
    "    imputer_dict,\n",
    "    oversampler_dict,\n",
    "    encoders_dict,\n",
    "    x_train_run,\n",
    "    y_train_run,\n",
    "    x_test_run,\n",
    "    y_test_run,\n",
    "    n_folds,\n",
    "    randomized_iteration,\n",
    "):\n",
    "\n",
    "    for model_name, model_hps in ml_models_dict.items():\n",
    "\n",
    "        for imputer_name, imputer_hps in imputer_dict.items():\n",
    "\n",
    "            for over_name, oversampler_hps in oversampler_dict.items():\n",
    "\n",
    "                for encoder_name, encoder_hps in encoders_dict.items():\n",
    "\n",
    "                    pipeline = make_main_pipeline(\n",
    "                        model_hps[\"model\"],\n",
    "                        imputer_hps[\"model\"],\n",
    "                        encoder_hps[\"model\"],\n",
    "                        oversampler_hps[\"model\"],\n",
    "                    )\n",
    "\n",
    "                    skf = StratifiedKFold(\n",
    "                        n_splits=n_folds, shuffle=True, random_state=random_seed\n",
    "                    )\n",
    "\n",
    "                    # Creating search space\n",
    "                    ml_model_hypers = {\n",
    "                        \"classifying__\" + str(k): v\n",
    "                        for k, v in model_hps[\"hyperparameters\"].items()\n",
    "                    }\n",
    "                    oversampler_hypers = {\n",
    "                        \"oversampling__\" + str(k): v\n",
    "                        for k, v in oversampler_hps[\"hyperparameters\"].items()\n",
    "                    }\n",
    "                    imputer_hypers =  {\n",
    "                        \"imputing__\" + str(k): v\n",
    "                        for k, v in imputer_hps[\"hyperparameters\"].items()\n",
    "                    }\n",
    "                    encoder_hypers =  {\n",
    "                        \"preprocessing__categorical__encoding__\" + str(k): v\n",
    "                        for k, v in encoder_hps[\"hyperparameters\"].items()\n",
    "                    }\n",
    "                    \n",
    "                    search_space = {\n",
    "                        **ml_model_hypers,\n",
    "                        **oversampler_hypers,\n",
    "                        **imputer_hypers,\n",
    "                        **encoder_hypers, \n",
    "                    }\n",
    "                    \n",
    "                    # Calculating search space size\n",
    "                    search_space_size = 1\n",
    "                    for k, v in search_space.items():\n",
    "                        search_space_size *= len(v)\n",
    "\n",
    "                    # Defining RandomizedSearch on search space\n",
    "                    randomized_tuner = RandomizedSearchCV(\n",
    "                        pipeline,\n",
    "                        search_space,\n",
    "                        n_iter=randomized_iteration,\n",
    "                        n_jobs=-1,\n",
    "                        random_state=random_seed,\n",
    "                        cv=skf,\n",
    "                        refit=\"f1\",\n",
    "                        scoring=scores,\n",
    "                        return_train_score=True,\n",
    "                    )\n",
    "\n",
    "                    print(\n",
    "                        \"Fitting following config with RandomizedSearch : \\nmodel : {0} \\\n",
    "                            \\noversampller : {1} \\nImputer : {2} \\nencoder : {3}\\n\".format(\n",
    "                            model_name, over_name, imputer_name, encoder_name\n",
    "                        ),\n",
    "                    )\n",
    "                    \n",
    "                    print(\"Training....\\n\")\n",
    "                    start_time = datetime.now()\n",
    "                    \n",
    "                    randomized_tuner.fit(x_train_run, y_train_run)\n",
    "                    \n",
    "                    print(\"Training is done.\\n\")\n",
    "                    end_time = datetime.now()\n",
    "                    \n",
    "                    total_time = end_time - start_time\n",
    "                    print(\"Execution time: {}\".format(total_time))\n",
    "                    \n",
    "                    # Making prediction on test data\n",
    "                    y_pred_run = randomized_tuner.predict(x_test_run)\n",
    "                    test_results = evalute_test_data(y_test_run, y_pred_run)\n",
    "\n",
    "                    save_results(\n",
    "                        tuner=randomized_tuner,\n",
    "                        total_exe_time=total_time,\n",
    "                        search_spcace_size=search_space_size,\n",
    "                        model_name=model_name,\n",
    "                        over_name=over_name,\n",
    "                        imputer_name=imputer_name,\n",
    "                        encoder_name=encoder_name,\n",
    "                        test_results=test_results,\n",
    "                    )\n",
    "\n",
    "                print(50 * \"-\")\n",
    "\n",
    "    print(\"All configs have run successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = make_metrics_for_tuning()\n",
    "encoders = get_encoders([\"BackwardDifferenceEncoder\"])\n",
    "imputers = get_imputer([\"KnnImputer\"])\n",
    "oversamplers = get_oversampler([\"SMOTE\"])\n",
    "models = get_ml_algo([\"XGB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting following config with RandomizedSearch : \n",
      "model : XGB                             \n",
      "oversampller : SMOTE \n",
      "Imputer : KnnImputer \n",
      "encoder : BackwardDifferenceEncoder\n",
      "\n",
      "Training....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best = run(scores = metrics,\n",
    "    ml_models_dict=models,\n",
    "    imputer_dict=imputers,\n",
    "    oversampler_dict=oversamplers,\n",
    "    encoders_dict=encoders, \n",
    "    x_train_run=x_train_copy,\n",
    "    y_train_run=y_train_copy,\n",
    "    y_test_run=y_test,\n",
    "    x_test_run=x_test,\n",
    "    n_folds=2,\n",
    "    randomized_iteration=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b825a7629194fa5a740a04d64edba515567a2497195d2d52d776a7beab3ada13"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
